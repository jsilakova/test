<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Banuba is a computer vision centric startup. Everything we do is around artificial intelligence, computer vision and other machine learning technologies.">
    <link rel="icon" type="image/png" href="assets/favicon/favicon.ico" sizes="16x16">
    <link rel="icon" href="assets/favicon/favicon-32.png" sizes="32x32">
    <link rel="icon" href="assets/favicon/favicon-192.png" sizes="192x192">
    <title>Banuba – AR: the future overlaid on today</title>
    <link href="css/all.css?v=1.0.94" rel="stylesheet">
  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <div class="container-fluid">
          <nav class="navbar navbar-dark navbar-expand-lg"><a class="navbar-brand" href="/"><img class="logotype" src="assets/logos/logo-banuba.svg" width="86" height="70" alt="BANUBA logotype"></a><a class="btn-navigation-open" href="#"><span class="icon-hamburger"><span></span><span></span><span></span><span></span></span></a>
            <div class="main-navigation">
              <ul class="navbar-nav">
                    <li class="nav-item"> <a class="nav-link" href="/sdk-tutorials">SDK demo</a></li>
                    <li class="nav-item active"> <a class="nav-link" href="/technology">Technology</a></li>
                    <li class="nav-item"> <a class="nav-link" href="/request-sdk-demo">Request demo</a></li>
              </ul>
            </div>
          </nav>
        </div>
      </header>
      <main class="main">
        <canvas id="nokey" width="100vw" height="100vh">Your Browser Don't Support Canvas, Please Download Chrome ^_^``</canvas>
        <div class="container-fluid">
          <section class="section section-content">
                <nav>
                  <ol class="breadcrumb">
                    <li class="breadcrumb-item"><a href="/">Back to Home</a>
                    </li>
                  </ol>
                </nav>
            <div class="mw-90">
              <div class="section-content-block">
                <h1 class="section-title">Banuba cutting-edge AR technology for&nbsp;mobile&nbsp;apps </h1>
                <h2>FaceAR SDK</h2>
                <p>FaceAR SDK incorporates a mix of technologies, including face detection and tracking, eye gaze detection, estimation of distance from the smartphone, recognition of skin and hair color and hair style, and emotion recognition, a synergy that creates substantial user benefits for various applications.</p>
                <p>Banuba offers a unique partnership opportunity for up to ten app developers across the world to create AR-enabled products.</p>
                <p>The partners will receive:</p>
                <ul class="content-list">
                  <li>Access to Banuba’s mobile AR SDK</li>
                  <li>Marketing and business development support</li>
                  <li>Funding for a joint venture as needed</li>
                </ul>
                <p>Among the first partners to take up the offer is game developer Inventain. Their team has received €1 million to develop their project jointly with us. This initiative is backed by major investors as part of an international programme to develop AI and AR technologies.</p>
                <p>The Banuba AR SDK is a unique product for mobile app developers. It is suitable even for constrained devices and combines several advanced technologies:</p>
                <ul class="content-list">
                  <li>3D face detection and tracking</li>
                  <li>3D face analysis, including detection of skin and hair color, and hair style</li>
                  <li>Separation from background objects</li>
                </ul>
              </div>
              <div class="section-content-block"><span class="section-subtitle">Banuba’s high performance is achieved thanks to:</span>
                <ul class="content-list">
                  <li>
                    <p><strong>An integrated 3D math model, which removes the need for 2D point identification</strong></p>
                    <p>Other solutions create filters by identifying 2D points on the face first, then applying nonlinear equations to create a 3D model of the head. </p>
                    <p>Banuba technology is different. It establishes a 3D model of the head directly, skipping the identification of 2D points. This makes the solution vastly more accurate. The 3D math model (Face Kernel ™), developed by Banuba, reduces all possible transformations to a limited number of variables.  </p>
                    <p>Meanwhile, direct creation of the 3D model leads to a high degree of precision. This is due to removing the need to perform 2D-to-3D transformation. <br><br></p>
                  </li>
                  <li>
                    <p><strong>Datasets tuned to work perfectly with Banuba algorithms</strong></p>
                    <p>Creating a suitable data set of face structures is a challenge for most providers. For example, some variables are not structured and remain subjective – such the classification of hairstyles.</p>
                    <p>To create such data sets, Banuba uses complementary systems. Semi-supervised metric learning and generative adversarial networks (GANs) work together. We use hand-crafted, hand-tuned code alongside compilers, harnessing human ability where necessary.</p>
                    <p>In machine learning, some say, jokingly, that a data set is more important than the algorithm. But this is perhaps not just a joke. Banuba doesn’t use pre-existing data sets – or, to be exact, Banuba uses them only as a part of its own data sets.  </p>
                    <p>This is because some data sets simply don’t exist. Where they do exist, a correctly constructed data set, if tuned to the learning algorithm, can significantly reduce the learning error. To create data sets, Banuba uses semi-supervised learning, a constraint form of active learning and domain adaptation methods.</p>
                    <p>Today, unfortunately, or perhaps fortunately, taking a data set, using your preferred machine learning method and compiling the result with the “-O3” key is no longer enough to develop a state-of-the-art solution. </p>
                    <p>Banuba has designed in-house math models to significantly simplify the effort. This cuts the execution time on a smartphone, reduces the learning time for the algorithm, and allows the use of a larger data set, which in turn improves the quality of the operation. Banuba uses a rather unconventional form of deep learning, mixing CNNs and different variations of Random Forests.  <br><br></p>
                  </li>
                  <li>
                    <p> <strong>Optimization for specific architectures, namely Apple A9, A10, A11 CPUs and Android</strong></p>
                    <p>In addition, Banuba has developed unconventional types of neural network layers, tuned to specific architectures, namely Apple A9, A10, A11 CPUs and Android. </p>
                    <p>All these features allow the Banuba AR SDK to be supported even by constrained devices, providing effective and fast performance on 90% of smartphones. Last but not least, the Banuba AR SDK will not drain battery like many applications. This significantly enhances the experience of those who use applications built with Banuba AR SDK. <br><br></p>
                  </li>
                </ul>
              </div>
              <div class="section-content-block">
                <h2>1. 3D Face Motion Tracking</h2>
                <h3>1.1 Face detection and head pose tracking</h3><img class="img-fluid section-image" src="assets/images/technology/image-01.jpg" alt="Face detection and head pose tracking image">
                <p>This technology detects the face and head-pose movements. Once the face has been detected, the algorithm switches to head-pose tracking mode by using the position of the head on the previous image as the initial approximant. If the face is lost, the algorithm switches back to face detection mode.</p>
                <p>Based on a directly inferred 3D model of the head (rather than one transformed from a 2D model), the technology is also operable even with a low SNR (signal to noise ratio) and poor lighting conditions. The incorporated model can forecast the appearance of the head in the subsequent frame, increasing stability and precision.</p>
                <p>The size of the data set is 300,000 faces.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>High performance (60 fps)</li>
                  <li>High quality</li>
                  <li>Extreme angles, ranging from –90 to +90</li>
                  <li>Efficient operation in poor lighting conditions</li>
                  <li>Operation with even with obstructions of up to 50% of the face</li>
                  <li>Stable detection and resistance to partial obstructions of the face, including glasses and complex haircuts</li>
                  <li>Depending upon needs, a 3D model of a face with 64 to 3,308 peaks is created</li>
                  <li>Supports 360 degrees rotation of the smartphone camera</li>
                  <li>Estimation of distance from the smartphone</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3 class="max-gap">Eye tracking and gaze detection</h3><img class="img-fluid section-image" src="assets/images/technology/image-02.jpg" alt="Eye tracking and gaze detection image">
                <p>Thanks to this technology, it is possible to both “track” a person’s gaze, and control a smartphone’s function with eye movements. An algorithm detects micro-movements of the eye with subpixel accuracy in real-time. Based on that data, a vector of movement is created.</p>
                <p>Banuba’s face recognition algorithm helps to measure the distance to various points on a scanned surface with a high degree of precision and to detect its shape. It can detect, for instance, whether the user’s eyes are open or closed.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>High degree of precision</li>
                  <li>Eye pupil detection and tracking</li>
                  <li>Eye states: open &amp; closed</li>
                  <li>Eye blinking</li>
                  <li>Attention tracking</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Facial motion capture</h3>
                <p>Face Motion Capture involves scanning facial movements and converting them to computer animation for movies, games, or real-time avatars. It can operate either in the real-time or based on the user’s preliminarily saved data or/and their face-motion models. Derived from the movements of real people, the technology results in more realistic and nuanced computer character animation than if the animation were created manually.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Fast execution – operates in real-time based on at least one or several frames. Each consequent frame may improve the model.</li>
                  <li>Can be integrated into the Face Recognition pipeline in order to develop precise models of users combining both visual similarity and the resemblance of facial gestures, emotions and other motion-related features.</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>3D face motion capture applications</h3>
                <ul class="content-list">
                  <li>Face recognition (biometrics)</li>
                  <li>Autofocus (photography)</li>
                  <li>Unconventional elements of the user interface, games</li>
                  <li>Measuring user interaction and engagement with in-app ads</li>
                  <li>Behavioral analytics of mobile UI/UX based on eye gaze tracking</li>
                  <li>Work style analysis: eyes blinking frequency, analysis of employee attention concentration, estimation of employee type of activity: typing, reading, calculation of onscreen time spent. <br><br></li>
                </ul>
              </div>
              <div class="section-content-block">
                <h2>2. 3D Face Analysis</h2>
                <h3>2.1 Face segmentation</h3>
                <p>Face segmentation is a specific computer vision task which assigns labels to facial features such as nose, mouth, eye, hair, etc., to each pixel in a face image. Our face segmentation techniques include complex cascaded machine learning algorithms in combination with color model and Monte Carlo approaches. This leads to precise detection of eyes, their structure (iris, pupil, eyeball, etc.) and the nose, ear, cheek, chin, mouth, lip eyebrow and forehead.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Access over a convenient API</li>
                  <li>Separate parts of the face can be detected for further analysis</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>2.2 Evaluation of anthropometric parameters</h3>
                <p>Facial anthropometry refers to the measurement of the individual facial features. Our novel algorithm automatically detects a set of anthropometric facial fiducial points that are associated with these features. This makes it possible to recognize refined facial patterns and recreate detailed semantics, mimics and anthropometrics.</p>
              </div>
              <div class="section-content-block">
                <h3>Features and applications</h3>
                <ul class="content-list">
                  <li>Access over a convenient API</li>
                  <li>Reconstruction of face geometry “cleaned” of mimics</li>
                  <li>Creation of caricature avatars in an instant</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3 class="max-gap">2.3 Skin and hair color</h3><img class="img-fluid section-image" src="assets/images/technology/image-03.jpg" alt="Skin and hair color image">
                <p>Banuba has developed a library for detecting hair and skin color for iOS. The area above the person’s forehead is used for hair color detection as it is less sensitive to head’s turns than other areas of hair. The technology recognises sharp intensity patterns above the hairline and analyzes color. There will be no such sharp intensity patterns for bald individuals, so our technology also allows us to detect the lack of hair.</p>
                <p>To detect skin color samples are taken from areas of the face that algorithms point to as specifically being face skin.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Precise color measurement</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Applications</h3>
                <ul class="content-list">
                  <li>Visual skin color and skin tone correction</li>
                  <li>Skin-related disorder detection</li>
                  <li>Virtual makeup</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>2.4 Hair style detection</h3>
                <p>This technology is based on convolutional neural networks. The learning sample selection consists images of men and women, categorised by their hairstyle. Learning is the process of matching an image with the most relevant hairstyle sample.</p>
                <p>Semi-supervised metrics learning was used for the creation of the data set. Subsequently, a GAN was trained for the expansion of the data set, upon which, the final network was trained. To improve quality, a loss function, specifically selected for the task, was used.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Detecting hair style, hair color</li>
                  <li>Changing hair color</li>
                  <li>Haircuts of any shape can be detected, regardless of hair length</li>
                  <li>The algorithm distinguishes with pixel precision between hair, face and other parts, such as beard, moustache or glasses</li>
                  <li>Matting is done with pixel precision. If the background is seen through the hair, it is detected as part of the hair.</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Applications</h3>
                <ul class="content-list">
                  <li>Virtual hair salon: trying out new hair colors and hair styles</li>
                  <li>Detecting hair style for 3D avatar creation</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3 class="max-gap">2.5 Emotion and expression recognition</h3><img class="img-fluid section-image" src="assets/images/technology/image-04.jpg" alt="Emotion and expression recognition image">
                <p>People’s emotions are reflected on their faces, and with the ability to “read a face” it is possible to deliver more personalized content. Our technology allows us to detect six basic emotions; anger, disgust, fear, happiness, sadness, and surprise.</p>
                <p>The variables of the model returned by the recognition algorithm can be used either directly, or transformed into parameters for another model, such as Facial Action Coding System (FACS), which detects muscle movements that correspond to specific emotions.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Real-time detection of anger, disgust, fear, happiness, sadness, and surprise</li>
                  <li>Access to a convenient API</li>
                  <li>Conversion of data to <a class="link" href="https://www.paulekman.com/" target="_blank"> Paul Ekman</a>'s <a  class="link" href="https://www.paulekman.com/product-category/facs/" target="_blank">Facial Action Coding System</a></li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Applications</h3>
                <ul class="content-list">
                  <li>Mood-related content in mobile applications, such as three-dimensional visual masks that reflect users’ feelings while they are communicating in mobile video chats</li>
                  <li>Targeted advertisement</li>
                  <li>Detection of tiredness and degree of stress</li>
                  <li>Use of emotional reactions to a product or content in empathic apps and advertisements</li>
                  <li>Creation of human-friendly digital products which react to human mimics</li>
                  <li>Detection of emotional states of patients for health purposes (helping schizophrenic patients)</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3 class="max-gap">2.6 Face Beautification</h3><img class="img-fluid section-image" src="assets/images/technology/image-05.jpg" alt="Face Beautification image">
                <p>This technology is used for creating visually beautified images of users. Anthropometric data and mimics are analyzed and corrected in real time.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Smoothing of skin</li>
                  <li>Correction of face skin tone</li>
                  <li>Whitening of eyes and teeth</li>
                  <li>Correction of face shape (make it slimmer, wider, increase/decrease eye size, change the shape of the nose and head proportions)</li>
                  <li>Change hair color</li>
                  <li>Improve face symmetry</li>
                  <li>Shape and color eyebrows</li>
                  <li>Correction of lip shape</li>
                  <li>Virtual makeup</li>
                  <li>Face morphing</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Improving the user’s look during video chat</li>
                  <li>Cosmetic surgery: ‘Before’ and ‘after’ visual aids during consultation with a patient</li>
                  <li>Cosmetics: application of  makeup / choosing which makeup product suits consumer</li>
                  <li>Demoing effects of face skin products</li>
                  <li>Fixing smartphone camera distortions</li>
                  <li>Make selfies more attractive</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h2>3. Separating user image from the background.</h2>
                <p>Banuba has developed a library to separate a person’s image from the background for Apple iOS.</p>
                <p>This technology can be used for the real-time replacement of backgrounds with both static and animated textures. Backgrounds can be changed into a number of optional presets during video calls or used as an engaging effect in advertising.</p>
                <p>The technology is based on convolutional neural networks, with color images as the input and a probability mask showing whether a pixel belongs to the class “person” or to the class “background.” This allows us to ensure high performance and results for real-time backgrounds.</p>
                <p>The problem of lack of data sets for separation of objects from the background is resolved by the creation of a small initial data set, which is increased by active learning and subsequent fine-tuning.</p>
                <p>A correctly selected data set helps to obtain optimal results and high-quality implementation.</p>
              </div>
              <div class="section-content-block">
                <h3>Features</h3>
                <ul class="content-list">
                  <li>Augmentation of raw output with classic algorithms for computer vision and signal filtration – lightweight post processing (deletion of small contours, precise definition of borders, additional image recognition).</li>
                  <li>Post-processing, including mating, filters and removal of small objects</li>
                  <li>Portrait mode and bokeh effect</li>
                </ul>
              </div>
              <div class="section-content-block">
                <h3>Applications</h3>
                <ul class="content-list">
                  <li>Replacement of an unsuitable background and noise removal</li>
                  <li>Animation effects in the background that are changed by the user as part of interactivity</li>
                  <li>Animated emotion-related background</li>
                  <li>Advertisements</li>
                  <li>Adding colors</li>
                  <li>Protection of privacy</li>
                  <li>360-degree background 2D and 3D for educational purposes, e.g. for mixed reality</li>
                  <li>‘Hollywood effects’ on a mobile phone</li>
                  <li>Replacing backgrounds for practical purposes (e.g. during a business call) or to entertain (e.g. jungle instead of a wall)</li>
                  <li>Editing “boring” backgrounds to create perfect videos</li>
                  <li>Removing unwanted objects or people from videos</li>
                </ul>
              </div>
            </div>
          </section>
              <section class="section section-contact">
                <div class="brand-holder">
                  <div class="brand-decor-content">
                    <div class="box bg-secondary contact-box-min">
                      <div id="contact-section">
                        <div class="contact-form-holder">
                          <div class="form-heading">
                            <h1 class="section-title section-title-white">Banuba can transform your&nbsp;business</h1>
                            <p>Just write your email and we will answer you</p>
                          </div>
                          <form class="form-inline contact-form form-light" action="#">
                            <div class="form-group">
                              <div class="form-control-holder">
                                <input class="form-control mr-2 js-contact-email" type="email" placeholder="Please enter your E-mail">
                                <div class="invalid-feedback">Please enter a valid E-mail address</div>
                              </div>
                              <button class="btn btn-outline-light js-email-submit" type="submit">Send</button>
                            </div>
                          </form>
                        </div>
                        <div class="contact-success">
                          <h1 class="section-title section-title-white">Successful sending!</h1>
                          <p>Thanks for your email.  </p>
                          <p>We will contact you as soon as possible.</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
          <div class="modal fade" id="modalError" tabindex="-1" role="dialog" aria-labelledby="modalError" aria-hidden="true">
            <div class="modal-dialog modal-dialog-centered" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h5 class="modal-title">Error</h5>
                  <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                </div>
                <div class="modal-body">
                  <p> Sorry, problem occured </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </main>
      <footer class="footer">
        <div class="container-fluid">
          <div class="decor-triangle-line"></div>
          <div class="footer-info-holder">
            <div class="footer-info">
              <div class="d-none d-md-block">
                <div class="footer-logo"><a href="/"><img src="assets/logos/logo-banuba.svg" width="160" height="auto" alt="BANUBA"></a></div>
                <p>Build smart apps easily with&nbsp;FaceAR&nbsp;SDK</p>
              </div>
              <div class="social-list"><a href="https://www.linkedin.com/company/banuba-development?originalSubdomain=ru" target="_blank"><img src="assets/icons/social/linkedin.svg" alt="linkedin" width="22" height="22"></a><a href="https://www.facebook.com/banubadevelopment/" target="_blank"><img src="assets/icons/social/facebook.svg" alt="facebook" width="12" height="21"></a><a href="https://twitter.com/BanubaFaceAR?lang=en" target="_blank"><img src="assets/icons/social/twitter.svg" alt="twitter" width="24" height="20"></a><a href="https://www.youtube.com/channel/UCnXZnhDdFv-1VC47PCY9Ptw" target="_blank"><img src="assets/icons/social/youtube.svg" alt="youtube" width="26" height="17"></a></div>
            </div>
            <div class="footer-info-links sdk-links">
              <h2 class="footer-title">SDK demo</h2>
              <ul class="footer-list">
                <li><a href="/face-tracking"> Face tracking and segmentation</a></li>
                <li><a href="/hair-segmentation"> Hair segmentation</a></li>
                <li><a href="/action-units-detection"> Action units detection (Avatar)</a></li>
                <li><a href="/occlusion-detection"> Occlusion detection</a></li>
                <li><a href="/glasses-detection"> Glasses detection</a></li>
                <li><a href="/eye-tracking"> Eye tracking</a></li>
                <li><a href="/hands-segmentation"> Hands segmentation</a></li>
                <li><a href="/beautification"> Beautification</a></li>
                <li><a href="/eyes-segmentation"> Eyes segmentation</a></li>
                <li><a href="/background-separation"> Background separation</a></li>
                <li><a href="/heart-rate-detection"> Heart rate detection</a></li>
              </ul>
            </div>
            <div class="footer-info-links">
              <h2 class="footer-title">Company</h2>
              <ul class="footer-list">
                <li><a href="/request-sdk-demo">Request&nbsp;demo</a></li>
              </ul>
            </div>
            <div class="footer-info-links">
              <h2 class="footer-title">Policy</h2>
              <ul class="footer-list">
                <li><a href="/terms" target="_blank">Terms&nbsp;of&nbsp;use</a></li>
                <li><a href="/policy" target="_blank">Policy</a></li>
              </ul>
            </div>
          </div>
          <div class="footer-copy"><span class="copyright">&copy; 2018 Banuba Development. <br class="d-block d-md-none"> All rights reserved.</span><span>Built with <span class="heart">🧡 </span>in Minsk</span></div>
        </div>
      </footer>
      <div class="modal fade" id="modalSuccessSend" tabindex="-1" role="dialog" aria-labelledby="modalError" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered modal-content-xl" role="document">
          <div class="modal-content">
            <div class="modal-header">
              <h5 class="modal-title">Successful sending!</h5>
              <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
            </div>
            <div class="modal-body">
              <div class="row align-items-center">
                <div class="col-sm-4"><img class="img-fluid img-center" src="assets/images/contact/success-hand.svg"></div>
                <div class="col-sm-8">
                  <p> Thanks for your email. </p>
                  <p> We will contact you as soon as possible.</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="modal fade" id="modalError" tabindex="-1" role="dialog" aria-labelledby="modalError" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
          <div class="modal-content">
            <div class="modal-header">
              <h5 class="modal-title">Error</h5>
              <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
            </div>
            <div class="modal-body">
              <p> Sorry, problem occured </p>
            </div>
          </div>
        </div>
      </div>
    </div><script src="js/jquery.min.js"></script><script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/slick.min.js"></script>
<script src="js/aos.js"></script>
<script src="js/main.js?v=1.0.92"></script>
  </body>
</html>